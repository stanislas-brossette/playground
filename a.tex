

Optimization on non-Euclidean Manifolds

The tangent space to M at point x

\graphicspath{{Chapter3-Manifolds/Figs/}{Chapter3-Manifolds/Figs/Humanoids2015/}}

%{{{ LIST OF CONTRIBUTIONS \section{List of contributions}
%\begin{itemize}
  %\item{Optimisation on Manifolds}
    %\begin{itemize}
      %\item Definition and examples of non-Euclidean manifolds
        %\begin{itemize}
          %\item $\mathbb{R}^n$
          %\item $SO(3)$
          %\item $S2$
          %\item Cartesian Products
        %\end{itemize}
      %\item Representation space vs Tangent space vs manifold
      %\item Local parametrization, notion of retractation
      %\item Computation of function's derivatives, differentiation of retractation
      %\item Hessian update on manifolds, notion of vector transport and application to BFGS and SR1 updates
      %\item Computation of distances, notion of logarithm
    %\end{itemize}
  %\item{Practical Implementation: PGSolver}
    %\begin{itemize}
      %\item General SQP algorithm adapted to local parametrization of manifolds.
      %\item Local QP and resolution with LSSOL
      %\item Feasibility problems and resolution with LSSOL
      %\item Acceptance criterion:
        %\begin{itemize}
          %\item Merit function
          %\item Filter
        %\end{itemize}
      %\item Globalization method:
        %\begin{itemize}
          %\item Line-Search
          %\item Trust-Region
            %\begin{itemize}
              %\item Notions of limit-Map vs trust-region vs problem's boundaries
              %\item Decisions of increase or decrease of trust region's size
              %\item Anisotrope trust-region
            %\end{itemize}
        %\end{itemize}
      %\item Hessian Update:
        %\begin{itemize}
          %\item BFGS
          %\item SR1
          %\item Fletcher LQNU
          %\item Individual vs grouped updates
          %\item Limited memory updates
        %\end{itemize}
      %\item Termination conditions:
        %\begin{itemize}
          %\item Satisfaction of KKT constraints
          %\item Null step
          %\item Failure of QP, FP, other\dots
        %\end{itemize}
      %\item Restoration Phase:
        %\begin{itemize}
          %\item Problem to solve: minimization of violation, relaxation of violated cstr
          %\item Exit condition: Feasible problem
          %\item Second-order correction phase
        %\end{itemize}
    %\end{itemize}
  %\item{Applications and testings of PGSolver on non-robotic problems}
    %\begin{itemize}
      %\item Point cloud fitting
      %\item Surface parametrisation
      %\item Cube stacks
      %\item Schitkowsky
    %\end{itemize}
%\end{itemize}
%}}}

%{{{ INTRODUCTION TO OPTIMIZATION ON MANIFOLDS
\section{Introduction to optimization on Manifolds}
\label{sec:introduction_to_optimization_on_manifolds}

Posture generation has been formulated as a problem over a Euclidean space.
Robots variable may however be more naturally expressed over non-Euclidean manifolds.
The archetypes for this are the rotation part of the root body of a humanoid robot, and ball joints, whose variables live in $SO(3)$.
Some typical tasks are also naturally formulated on different manifolds.
For example, for making contact with any object that can be mapped on a sphere, the contact point position for this object can be parametrized in $S2$~\cite{escande:icra:2016}.
Human shoulder can be elegantly parametrized on $S2\times\mathbb{R}$, as proposed in~\cite{baerlocher}.

Formulating the problem over $\mathbb{R}^n$ leads either to discontinuities that can prevent the convergence of the optimization solver, or to cumbersome writing to specify that the variable is actually living on a manifold (see~\cite{bouyarmane:humanoids:2012}).

For example, let us consider an optimization problem over the $SO(3)$ manifold:

\begin{align}
\label{eq:pb_on_SO3}
  \minimize_{x \in SO(3)} & \quad f(x)\\
  \text{subject to}&
  \begin{array}{lr}
    l \leq c(x) \leq u \nonumber
  \end{array}
\end{align}

$SO(3)$ is a 3-dimensional manifold.
As such, it can be parametrized \emph{locally} by $3$ variables, for example, a choice of Euler angles, but any such parametrization necessarily exhibits singularities when taken as a global map (e.g.\ gimbal lock for Euler angles), which can be detrimental to our optimization process.

For this reason, when addressing $SO(3)$ with classical optimization algorithms, it is often preferred to use one of the two following parametrizations:
\begin{itemize}
    \item unit quaternion, \emph{i.e.} an element $q$ of $\mathbb{R}^4$ with the additional constraint $\left\|q\right\| = 1$,
    \item rotation matrix, \emph{i.e.} an element $R$ of $\mathbb{R}^{3 \times 3}$ (or equivalently $\mathbb{R}^9$) with the additional constraints $R^T R = I$ and $\det{R} \geq 0$.
\end{itemize}

Then, if we use the unit quaternion parametrization, the problem~\ref{eq:pb_on_SO3} becomes:
\begin{align}
\label{eq:pb_on_SO3_quaternion}
  \minimize_{q \in \mathbb{R}^4} & \quad f(q)\\
  \text{subject to}&
  \left\{\begin{array}{lr}
    {l} \leq c(q) \leq {u} \nonumber\\
    \|q\| - 1 = 0
  \end{array}\right.
\end{align}

The problem to solve has 4 dimensions (to represent a 3-dimensional manifold), and has an additional constraint that is entirely due to our formulation choice.
With this formulation, it is guaranteed that the solution $q^*$ is a unit quaternion, but not that all the iterates $q_k$ along the optimization process have a unit norm.
During the optimization process, at each iteration, an increment $\delta$ is computed by solving a quadratic problem that approximates~\ref{eq:pb_on_SO3_quaternion} locally around $q_k$.
In particular, this quadratic problem approximates the constraints linearly, thus, for any step $\delta$ not null, from a unit-norm quaternion iterate $q_k$, the next iterate $q_{k+1} = q_k + \delta$ does not respect the unit-norm constraint (it respects the \emph{linearization} of the unit-norm constraint).
So the quaternion $q_{k+1}$ does not represent a rotation.
Some additional treatment needs to be implemented.
For example, normalizing the quaternion at each iteration, then it represents a rotation, but this normalization must be taken into account in the constraints, cost functions, and their differentiations, which is an additional programming burden.
Similar issues can be found with the $\mathbb{R}^{3\times 3}$ matrix representation.

The alternative is to use optimization software working natively with manifolds~\cite{brossette:Humanoids:2015}\cite{absil:book:2008} and solve the optimization problem as it is written in~\ref{eq:pb_on_SO3}.
It has an immediate advantage: we can write directly the problem without the need to add any parametrization-related constraints.
Working directly with manifolds also has the advantage that at each iteration, the variables of the problem represent an element of the manifold.
This is not the case with the other formulations we discussed, as the (additional) constraints are guaranteed to be satisfied only at the end of the optimization process.
Having intermediate values naturally staying on the manifold can be useful to evaluate additional functions that pre-suppose it (additional constraints, external monitoring $\ldots$).
It can also be leveraged for real-time applications where only a short time is allocated repeatedly to the optimization, so that when the optimization process is stopped after a few iterations, the output is valid in the sense that it is always a point of the manifold.

With non-manifold formulations, at any given iteration, the parametrization-related constraints can be violated, thus, the variables might not lie in the manifold.
It is then needed to project them on it.
Denoting $\pi$ the projection (for example $\pi = \frac{q}{\left\|q\right\|}$ in the unit quaternion formulation), to evaluate a function $f$ on a manifold, we need to compute $f \circ \pi$.
If further the gradient is needed, that projection must also be accounted for (authors in~\cite{bouyarmane:humanoids:2012} explain that issue in great details for robotics problems with free-floating bases).

In this chapter, we present a new nonlinear constrained optimization solver able to work on generic smooth manifolds.
We take inspiration from the approach used for unconstrained optimization on manifold~\cite{absil:book:2008} and adapt it to constrained optimization.
To the best of our knowledge, constrained optimization on manifold has drawn few research for now.
This is likely due to the fact that in most problems the only constraint is to be on the manifold, in that case, unconstrained optimization on manifold is enough.
We are only aware of the work of Schulman \emph{et al.}~\cite{Schulman2014}, where the authors explain the adaptation of their solver to work on $SE(3)$.
This adaptation is however not valid for general manifolds without more care about hessian computation.

A background motivation for this work is to have our own optimization solver, instead of a black box.
We will now be able to specialize the solver for robotic problems, by leveraging modeling properties and approximations, for a gain in time and robustness.
%We also look forward to using this solver for problems with a varying number of constraints along the iterations (such as when complex collision constraints are considered).

%}}}
%{{{ OPTIMIZATION ON MANIFOLDS
\section{Optimization on Manifolds}
\label{sec:optimization_on_manifolds}

In this section, we describe a Sequential Quadratic Programming (SQP) approach~\cite{nocedal:book:2006} to solve the following non-linear constrained optimization program

\begin{align}
\label{eq:optim_problem}
  \minimize_{x \in \mathcal{M}} & \quad f(x)\\
  \text{subject to}&
  \begin{array}{lr}
    l \leq c(x) \leq u \nonumber
  \end{array}
\end{align}

where $\mathcal{M}$ is a $n$-dimensional smooth manifold and $c$ is a $m$-dimensional real-valued function.

%{{{ REPRESENTATION PROBLEM
\subsection{Representation problem}
When $\mathcal{M} = \mathbb{R}^n$, the problem~(\ref{eq:optim_problem}) is solved iteratively, starting from an initial guess $x_0$ and performing successive steps $x_{i+1} = x_i + {\bf p_i}$ where ${\bf p_i}$ is the increment found at the $i$-th iteration, until convergence is achieved.
The strategy to compute ${\bf p_i}$ depends on the solver.

This classical scheme cannot be readily applied to optimization over non-Euclidean manifolds.
First of all, only (a subset of) the real numbers can be stored in computers.
To manipulate elements of $\mathcal{M}$ we need to choose a way to represent them in memory.
This boils down to choosing a representation space $\mathbb{E} = \mathbb{R}^r$ (with $r \geq n$) and a map
\begin{equation}
  \psi\ :\
  \begin{array}{ccc}
    x & \reduce{\mapsto}{6} & \mathbf{x} \\
    \mathcal{M} & \reduce{\rightarrow}{6} & \mathbb{E}
  \end{array}\nonumber%
\end{equation}
In the following, we identify $\mathcal{M}$ with the set $\psi(\mathcal{M}) \subseteq \mathbb{E}$.

With this representation, it is tempting to simply transform problem~(\ref{eq:optim_problem}) as an optimization over $\mathbb{R}^r$ with objective $f \circ \psi^{-1}$ and constraint $c \circ \psi^{-1}$, and solve it with a usual solver.
But depending on the representation choice, one of the two following problems arises:\\
(i) $r=n$, then it is not possible in the general non-Euclidean case to find $\psi$ without derivative discontinuities.
This can lead to critical convergence problems, \\
(ii) $r>n$, then most elements of $\mathbb{E}$ do not represent an element of $\mathcal{M}$ %($\psi(\mathcal{M})$ is a measure-zero subset of $\mathbb{E}$)
and $\psi$ cannot be surjective.
Constraints need to be added to force the solution on $\mathcal{M}$.
As a result, the problem has more variables and constraints w.r.t (i).
Moreover, the additional constraints are unlikely to be met along the iteration process (even if $x_i$ is an element of $\mathcal{M}$, $x_i+{\bf p_i}$ is likely not, as nothing enforces it).
This means that in order to evaluate $f \circ \psi^{-1}$ and $c \circ \psi^{-1}$ at a given $x_i$, one has to project it on $\psi(\mathcal{M})$ first, effectively computing $f \circ \psi^{-1} \circ \pi$ and $c \circ \psi^{-1} \circ \pi$, where $\pi$ is the projection.
The composition by $\pi$ is an additional burden in programming (see e.g.\ in~\cite{bouyarmane:humanoids:2012a}).
Figure~\ref{fig:stepOnSphere} illustrates the difference between a step through $\varphi$ in optimization on manifolds and a step followed by a projection as is done in classical optimization.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.5\linewidth]{Humanoids2015/stepOnSphere.pdf}
  \caption{StepOnSphere}
\label{fig:stepOnSphere}
\end{figure}

As a simple example, the set of 3D-rotations $SO(3)$ is a manifold of dimension $3$.
The following (classical) choices can be made
\begin{itemize}
  \item Rotation matrix ${\bf R} \in \mathbb{R}^{3\times 3} \approx \mathbb{R}^9$, additional constraints: $\{{\bf R}^t{\bf R} = I\ ,\ \det({\bf R})=1\}$, projection by orthogonalization,
  \item Quaternion ${\bf q} \in \mathbb{R}^4$, additional constraints: $\{ \left\|{\bf q}\right\|=1\}$, projection $\pi({\bf x}) = {\bf x}/\left\|{\bf x}\right\|$,
  \item Euler angles ($\mathbb{E} = \mathbb{R}^3$), singularities when reaching gimbal lock.
\end{itemize}

%}}}
%{{{ LOCAL PARAMETRIZATION
\subsection{Local parametrization}
By definition, there is always, at a point $x$ of a smooth $n$-dimensional manifold $\mathcal{M}$, a smooth map $\varphi_x$ between an open set of $T_x\mathcal{M}\approx \mathbb{R}^n$, the tangent space to $\mathcal{M}$ at $x$, and a neighborhood of $x$ in $\mathcal{M}$, with $\varphi_x(0) = x$.

\begin{equation}
  \varphi_x\ :\
  \begin{array}{ccc}
    \mathbf{z} & \reduce{\mapsto}{6} & \varphi_x(\mathbf{z}) \\
    T_x\mathcal{M} & \reduce{\rightarrow}{6} & \mathcal{M}
  \end{array} \nonumber%
\end{equation}

$T_x\mathcal{M}$ can be identified with $\mathbb{R}^n$, but in some cases, it needs to be considered as a hyperplan of a higher dimensionality space.
For example in figures~\ref{fig:stepOnSphere} and~\ref{fig:phimap}, $T_x\mathcal{M}$ is a 2-dimensional hyperplan embedded in $\mathbb{R}^3$.
We denote $T_x\mathbb{E}$ the representation space of $T_x\mathcal{M}$.
This gives us a local parametrization for $\mathcal{M}$.
The driving idea of the optimization on manifolds is to change the parametrization at each iteration.
Applying this idea, we can reformulate Problem~(\ref{eq:optim_problem}) around $x_i$ as
\begin{align}
\label{eq:local_problem}
\minimize_{{\bf z} \in T_{x_i}\mathcal{M}} & \quad f \circ \varphi_{x_i}({\bf z}) \\
  \text{subject to}&
  \begin{array}{rcl}
    {l} \leq & \reduce{c \circ \varphi_{x_i}({\bf z})}{8}& \leq {b} \nonumber
  \end{array}
\end{align}
This is an optimization problem on $\mathbb{R}^n$.
If we perform one iteration of a classical solver starting from ${\bf z_0} = 0$, we get an iterate ${\bf z_1}$, which corresponds to the iterate $x_{i+1} = \varphi_{x_i}({\bf z_1})$.
We can then reformulate Problem~(\ref{eq:optim_problem}) around $x_{i+1}$, perform a new iteration and repeat the process until convergence.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=.9\linewidth]{Humanoids2015/manifold.pdf}
  \caption{There are many possible choices for $\varphi_{x}$ but not all yield a curve $\varphi_{x}(t{\bf z})$ which is going in the same direction as ${\bf z}$: $\varphi_{1}$ and $\varphi_{2}$ are correct choices, $\varphi_{3}$ is not.}
\label{fig:phimap}
\end{figure}

However, convergence cannot be achieved without care on the choice of $\varphi_{x_i}$: it must be such that for any ${\bf z}$, the curve $t \mapsto \varphi_{x_i}(tz)$ is tangent to ${\bf z}$, see Fig.~\ref{fig:phimap}, so that the update $x_{i+1} = \varphi_{x_i}({\bf z_1})$ is made in the direction given by ${\bf z_1}$.

The exponential map is a good theoretical candidate, but it is often impractical or expensive to compute.
Depending on the manifold, cheaper maps can be chosen.

With the iterative formulation approach described above, we do not have any parametrization issue, do not need additional constraints, and have the minimum number of optimization parameters.
But we still need a map $\psi$ and real space $\mathbb{E}$ to represent the $x_i$ and keep track of them in a global way.
The ${\bf x_i}$ are guaranteed to be on $\mathcal{M}$ so we can choose a representation with $r>n$ where $\psi$ is singularity-free without any drawback.
Also, the programmer can write the function $f' = f \circ \psi^{-1}$ as if it was a function from $\mathbb{E}$ to $\mathbb{R}$ without the need to project on $\psi(\mathcal{M})$ first (same goes for $c' = c \circ \psi^{-1}$).
For example if $\mathcal{M} = SO(3)$ and $\mathbb{E} = \mathbb{R}^{3\times 3}$, ${\bf x_i}$ is automatically a rotation matrix and can be used directly as such when writing the function.

%}}}
%{{{ LOCAL SQP ON MANIFOLDS
\subsection{Local SQP on manifolds}
\label{local_sqp_on_manifolds}
We choose to adopt an SQP approach to solve our problem.
We first define the Lagrangian function
\begin{equation}
  \mathcal{L}_x ({\bf z}, \lambda) = f\circ \varphi_x({\bf z}) - \lambda^T c \circ \varphi_x({\bf z})
\end{equation}
with $\lambda \in \mathbb{R}^m$ the vector of Lagrange multipliers, and note $H_k$ the Hessian matrix $\nabla_{zz}^2 \mathcal{L}_{x_k}$.
Taking ${\bf z_0} = 0$, the $k$-th SQP step for Problem~(\ref{eq:local_problem}) is computed by solving the following quadratic program:

\begin{align}
  \label{eq:SQPStep}
  \minimize_{\bf z \in \mathbb{R}^n } & \quad {\frac{\partial f\circ \varphi_{x_k}}{\partial {\bf z}}(0)}^T {\bf z } + \frac{1}{2} {\bf z }^T H_k{\bf z }\\
  \text{subject to}&
  \begin{array}{lr}
    \text{l} \leq c\circ \varphi_{x_k}(0) + \frac{\partial c\circ \varphi_{x_k}}{\partial {\bf z}}(0) {\bf z }\leq \text{u}\\
  \end{array} \nonumber%
\end{align}

The basic SQP approach adapted to manifolds can be summarized as follows
\begin{enumerate}
  \item set $k=0$ and $x_k$ to the initial value
  \item compute ${\bf z}$ from Problem~(\ref{eq:SQPStep}) for current $x_k$
  \item set $x_k = \varphi_{x_k}({\bf z})$
  \item if convergence is not yet achieved go-to step 2
\end{enumerate}

Computations of function values and derivatives are based on the fact that $f \circ \varphi = f' \circ \psi \circ \varphi$ (and same for $c$), and
\begin{align}
  f'\ :\
  \begin{array}{ccc}
    \mathbb{E} & \reduce{\rightarrow}{6} & \mathbb{R}
  \end{array} \nonumber\\
  \psi\circ\varphi:
  \begin{array}{ccc}
    \mathbb{R}^n & \reduce{\rightarrow}{6} & \mathbb{E}
  \end{array} \nonumber%
\end{align}
are representable functions (whereas $f$, $\psi$ and $\varphi_x$ are not, due to the fact that they feature $\mathcal{M}$ as input or output).
The gradient of $f \circ \varphi_x$ is
\begin{align}
  \frac{\partial f\circ\varphi_x}{\partial {\bf z}}=
  \frac{\partial f'}{\partial y}(\psi\circ\varphi_x)\times
  \frac{\partial (\psi\circ\varphi_x)}{\partial {\bf z}}
\end{align}

$\frac{\partial f'}{\partial y}$ denotes the gradient of $f'$ with respect to an element of $\mathbb{E}$, which is the derivative that is usually calculated for use in classical optimization schemes.

%}}}
%{{{ DESCRIPTION OF NON-EUCLIDEAN MANIFOLDS
\subsection{Description of non-Euclidean manifolds}
\label{sub:examples_on_non_euclidean_manifolds}

For each elementary manifold $\mathcal{M}$, we need to define a set of elements and operations.
Those need only to be implemented once for each elementary manifold (it is then trivial to get those functions for Cartesian products of manifolds).
The composition with $f'$ and $c'$ is done automatically.
The expression of those functions is adapted from~\cite{boumal:jmlr:2014}.

\paragraph{Retractation:}
We need a retractation operation $\psi\circ\varphi_x$ (that we denote simply $\varphi_x$) and its derivative.
During the optimization process, we need the expression of the derivatives of $\varphi_x$ in order to compute the gradients of the cost function and constraints at the beginning of each iteration to later compute the approximated quadratic problem to be solved in that iteration.
Because we change the parametrization of our problem to be centered on $x_k$ at each iteration, we only ever need to evaluate the gradient of $\varphi_x$ for $\mathbf{z}=0$, $\frac{\partial \varphi_x}{\partial \mathbf{z}}(0)$.
In many cases, this quantity is invariant w.r.t $x$ and can be computed once and for all for each manifold.

\paragraph{Pseudo-Logarithm and distances:}
It is necessary to be able to compute distances on manifolds.
For that, we define the pseudo-logarithm (aka pseudolog), which is the inverse of the retractation operator.
Note that the logarithm map is the inverse of the exponential map.
\begin{equation*}
  \zeta_x:\mathcal{M}\rightarrow T_x\mathcal{M}
\end{equation*}
\begin{equation*}
  \forall (x,y)\in \mathcal{M}\times\mathcal{M},\ \mathbf{z}:=\zeta_x(y)\ \text{is such that}\ \varphi_x(\mathbf{z}) = y
\end{equation*}
The pseudolog operator gives the vector of $T_x\mathcal{M}$ to go from $x$ to $y$.
It is used to compute the (pseudo-) distance between two points of $\mathcal{M}$
\begin{equation*}
  \dist(x,y) = \|\zeta_x(y)\|
\end{equation*}

\paragraph{Vector Transport:}
To compare two vectors $\mathbf{v}_1$ and $\mathbf{v}_2$ defined in tangent spaces of different points, respectively, $T_{x_1}\mathcal{M}$ and $T_{x_2}\mathcal{M}$, it is necessary to transport $\mathbf{v}_1$ into $T_{x_2}\mathcal{M}$.
Figure~\ref{fig:transport} illustrates the transport of a vector $\mathbf{v}_1$ from $T_{x_1}\mathcal{M}$ to $T_{x_2}\mathcal{M}$.
$\mathbf{v}_2$ can then be compared to the transported $\mathbf{v}_1$: $\mathcal{T}_{x_1,\mathbf{z}}(\mathbf{v}_1)$.
This operation will come in handy for the computation of Hessian approximations explained later in this chapter.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\linewidth]{transport.pdf}
  \caption{Vector transport on a non-Euclidean manifold}
\label{fig:transport}
\end{figure}

%\paragraph{Projections:} A projection operator on $\mathcal{M}$, as well as one on $T_x\mathcal{M}$ can come in handy, especially to help eliminate some numerical errors when necessary.

\paragraph{Limits on tangent map:} The tangent map can present some singularities, thus it is necessary to limit the length of steps made through retractation to the validity region of each manifold.

To summarize, for each elementary manifold $\mathcal{M}$, we need to implement the following elements:
\begin{itemize}
  \item Tangent Space at point $x$, $T_x\mathcal{M}$
  \item Embedding spaces $\mathbb{E}$ and $T_x\mathcal{M}$
  \item Retractation operator $\varphi:\ (x,\mathbf{z}) \rightarrow \varphi_x(\mathbf{z})$
  \item Gradient of the retractation operator $\partial \varphi(x):\rightarrow \frac{\partial \varphi_x}{\partial \mathbf{z}}(0)$
  \item Pseudo-logarithm operator $\zeta:\ (x,y) \rightarrow \zeta_x(y)$
  \item Gradient of pseudo-logarithm operator $\frac{\partial \zeta_x}{\partial y}(y)$
  \item Transport operator $\mathcal{T}:\ (x,\mathbf{z}, \mathbf{v})\rightarrow \mathcal{T}_{x,\mathbf{z}}(v)$
  \item Projection from $\mathbb{E}$ on $\mathcal{M}$, $\pi_\mathcal{M}$
  \item Projection from $T_x\mathbb{E}$ on $T_x\mathcal{M}$, $\pi_{T_x\mathcal{M}}$
  \item Limits of the tangent map, $\lim$
\end{itemize}

%{{{ THE REAL SPACE MANIFOLD $\MATHBB{R^N$}
\subsubsection{The Real Space manifold $\mathbb{R}^n$}
\label{ssub:the_real_space}
Since $\mathbb{R}^n$ is a Euclidean manifold, the operations that we use on it are straightforward.

\begin{table} [H]
\caption{Description of the $\mathbb{R}^n$ manifold}
\centering
\begin{tabular}{cc}
  \toprule
  $\mathcal{M}$ & $\mathbb{R}^n$ \\
  \midrule
  $\mathbb{E}$ & $\mathbb{R}^n$ \\
  \midrule
  $T_x\mathcal{M}$ & $\mathbb{R}^n$ \\
  \midrule
  $T_x\mathbb{E}$ & $\mathbb{R}^n$ \\
  \midrule
  $\varphi_x(\mathbf{z})$ & $\mathbf{x} + \mathbf{z}$ \\
  \midrule
  $\partial \varphi_x(0)$ & $\mathbb{I}_n$ \\
  \bottomrule
\end{tabular}
\quad
\begin{tabular}{cc}
  \toprule
  $\zeta(x,y)$ & $\mathbf{y} - \mathbf{x}$ \\
  \midrule
  $\frac{\partial \zeta_x}{\partial y}(x)$ & $\mathbb{I}_n$ \\
  \midrule
  $\mathcal{T}(x,\mathbf{z}, \mathbf{v})$ & $\mathbf{v}$ \\
  \midrule
  $\pi_\mathcal{M}(\mathbf{x})$ & $\mathbf{x}$ \\
  \midrule
  $\pi_{T_x\mathcal{M}}(\mathbf{z})$ & $\mathbf{z}$ \\
  \midrule
  $\lim$ & $\|\mathbf{v}\| \leq \infty$ \\
  \bottomrule
\end{tabular}
\end{table}
%}}}
%{{{ THE 3D ROTATION MANIFOLD SO(3): MATRIX REPRESENTATION
\subsubsection{The 3D Rotation manifold $\mathbf{SO(3)}$: Matrix representation}
\label{ssub:the_3d_rotation_manifold_matrix_representation}

An element $x$ of $SO(3)$ is represented by $\mathbf{x}=\psi(x)$ in $\mathbb{R}^{3\times 3}$ by:
\begin{equation}
  x\in SO(3),\ \mathbf{x} =\begin{bmatrix}
    x_{00} & x_{01} & x_{02} \\
    x_{10} & x_{11} & x_{12} \\
    x_{20} & x_{21} & x_{22} \\
  \end{bmatrix}
\end{equation}

We recall the operators
\begin{equation}
\widehat{.}: \begin{bmatrix}
  \omega_0\\\omega_1\\\omega_2\\
\end{bmatrix}
\rightarrow
\begin{bmatrix}
  0 & -\omega_2 & \omega_1 \\
  \omega_2 & 0 & -\omega_0 \\
  -\omega_1 & \omega_0 & 0\\
\end{bmatrix}
\end{equation}
And its inverse:
\begin{equation}
\widecheck{.}: \begin{bmatrix}
    x_{00} & x_{01} & x_{02} \\
    x_{10} & x_{11} & x_{12} \\
    x_{20} & x_{21} & x_{22} \\
\end{bmatrix}
\rightarrow
\begin{bmatrix}
  x_{21}\\x_{02}\\x_{10}\\
\end{bmatrix}
\end{equation}


The exponential map is known as the Rodrigues formula:
\begin{equation}
  \forall \mathbf{v}\in\mathbb{R}^3,\ \exp(\mathbf{v}) = \mathbb{I}_3 +
  \frac{\sin \|\mathbf{v}\|}{\|\mathbf{v}\|} \hat{\mathbf{v}} +
  \frac{1-\cos \|\mathbf{v}\|}{\|\mathbf{v}\|^2} \hat{\mathbf{v}}^2
\end{equation}

Note that when $\|\mathbf{v}\|$ is small, we make the following replacements to avoid numerical instability.
It is important to ensure the precision of the retractation near zero because in an optimization process, many small steps are taken, especially when close to the solution.

\begin{align}
 \frac{\sin \|\mathbf{v}\|}{\|\mathbf{v}\|} & = 1-\frac{\|\mathbf{v}\|}{6}\\
 \frac{1-\cos \|\mathbf{v}\|}{\|\mathbf{v}\|^2} & = 0.5 - \frac{\|\mathbf{v}\|}{24}
\end{align}

And the logarithm is computed as follows (see~\cite{merlhiot:thesis:2009}):
\begin{align}
\begin{split}
  \forall R\in\psi(SO(3)),\ f(R) &=
  \left\{ \begin{matrix}
  0 & \text{if }Tr(R) = 3 \\
  \frac{\arccos\left(\frac{Tr(R)-1}{2}\right)}{2\sin\left(\arccos\left(\frac{Tr(R)-1}{2}\right)\right)}\left(R-R^T\right) & \text{otherwise} \\
  \end{matrix} \right.\\
  \log(R) &= \widecheck{f\left(R\right)}
\end{split}
\end{align}

\begin{table} [H]
\caption{Description of the $SO(3)$ manifold with matrix representation}
\centering
\begin{tabular}{cc}
  \toprule
  $\mathcal{M}$ & $SO(3)$ \\
  \midrule
  $\mathbb{E}$ & $\mathbb{R}^{3\times 3}$ \\
  \midrule
  $T_x\mathcal{M}$ & $\mathbb{R}^3$ \\
  \midrule
  $T_x\mathbb{E}$ & $\mathbb{R}^3$ \\
  \midrule
  $\psi(x) = \mathbf{x}$ & $ \begin{bmatrix}
    x_{00} & x_{01} & x_{02} \\
    x_{10} & x_{11} & x_{12} \\
    x_{20} & x_{21} & x_{22} \\
  \end{bmatrix} $ \\
  \midrule
  $\varphi_x(\mathbf{z})$ & $\mathbf{x}\exp(\mathbf{z})$ \\
  \bottomrule
\end{tabular}
\quad
\begin{tabular}{cc}
  \toprule
  $\partial \varphi_x(0)$ & see Appendix~\ref{eq:diffRetrSO3Matrix} \\
  \midrule
  $\zeta(x,y)$ & $\log(\mathbf{x}^T\mathbf{y})$ \\
  \midrule
  $\frac{\partial \zeta_x}{\partial y}(x)$ & see Appendix~\ref{eq:diffLogSO3Matrix} \\
  \midrule
  $\mathcal{T}(x,\mathbf{z}, \mathbf{v})$ & $\mathbf{v}$ \\
  \midrule
  $\pi_\mathcal{M}(\mathbf{x})$ & Q from QR decomposition of $\mathbf{x}$ \\
  \midrule
  $\pi_{T_x\mathcal{M}}(\mathbf{z})$ & $\mathbf{z}$ \\
  \midrule
  $\lim$ & $\|\mathbf{v}\| \leq \pi$ \\
  \bottomrule
\end{tabular}
\end{table}

%}}}
%{{{ THE 3D ROTATION MANIFOLD SO3 QUATERNION REPRESENTATION
\subsubsection{The 3D Rotation manifold $\mathbf{SO(3)}$: Quaternion representation}
\label{ssub:the_3d_rotation_manifold_quaternion_representation}
An element $x$ of $SO(3)$ is represented by $\mathbf{q}=\psi(x)$ in $\mathbb{R}^{4}$ by:
\begin{equation}
  x\in SO(3),\ \mathbf{q} =\begin{bmatrix}
    q_{w}\\
    q_{x}\\
    q_{y}\\
    q_{z}\\
  \end{bmatrix}
  =\begin{bmatrix}
    q_{w}\\
    \mathbf{q_{vec}}\\
  \end{bmatrix}
\end{equation}

The exponential map is:
\begin{equation}
  \exp\ :\left|
  \begin{array}{ccc}
    \mathbb{R}^3 & \rightarrow & \mathbb{R}^4 \\
    \mathbf{z} & \mapsto & \begin{bmatrix}
      \cos \left( \frac{\|\mathbf{z}\|}{2} \right)\\
      \sin \left( \frac{\|\mathbf{z}\|}{2} \right) \frac{\mathbf{z}}{\|\mathbf{z}\|}\\
    \end{bmatrix} \\
  \end{array} \nonumber%
  \right.
\end{equation}

Note that when $\|\mathbf{v}\|$ is small, we make the following replacements to avoid numerical instability.

\begin{equation}
  \exp(\mathbf{z}) = \begin{bmatrix}
    1 -\frac{\|\mathbf{z}\|}{8} + \frac{{\|\mathbf{z}\|}^2}{384}\\
    \left(0.5 - \frac{\|\mathbf{z}\|}{48} + \frac{{\|\mathbf{z}\|}^2}{3840}\right)\mathbf{z}
  \end{bmatrix}
\end{equation}

And the logarithm is:
\begin{equation}
  \log\ :\left|
  \begin{array}{ccc}
    \mathbb{R}^4 & \rightarrow & \mathbb{R}^3 \\
    q & \mapsto & \arctan \left( \frac{2 \|\mathbf{q_{vec}}\| q_w}{q_w^2 - {\|\mathbf{q_{vec}\|}^2}} \right) \frac{\mathbf{q_{vec} } }{\|\mathbf{q_{vec}}\|} \\
  \end{array}
  \right.
\end{equation}


\begin{table} [H]
\caption{Description of the $\mathbf{SO(3)}$ manifold with quaternion representation}
\centering
\begin{tabular}{cc}
  \toprule
  $\mathcal{M}$ & $SO(3)$ \\
  \midrule
  $\mathbb{E}$ & $\mathbb{R}^{4}$ \\
  \midrule
  $T_x\mathcal{M}$ & $\mathbb{R}^3$ \\
  \midrule
  $T_x\mathbb{E}$ & $\mathbb{R}^3$ \\
  \midrule
  $\varphi_x(\mathbf{z})$ & $\mathbf{x}\exp(\mathbf{z})$ \\
  \midrule
  $\partial \varphi_x(0)$ & see Appendix~\ref{eq:diffRetrSO3Quat} \\
  \bottomrule
\end{tabular}
\quad
\begin{tabular}{cc}
  \toprule
  $\zeta(x,y)$ & $\log(\mathbf{x}^{-1}\mathbf{y})$ \\
  \midrule
  $\frac{\partial \zeta_x}{\partial y}(x)$ & see Appendix~\ref{eq:diffLogSO3Quat} \\
  \midrule
  $\mathcal{T}(x,\mathbf{z}, \mathbf{v})$ & $\mathbf{v}$ \\
  \midrule
  $\pi_\mathcal{M}(\mathbf{x})$ & $\frac{\mathbf{x}}{\|\mathbf{x}\|}$ \\
  \midrule
  $\pi_{T_x\mathcal{M}}(\mathbf{z})$ & $\mathbf{z}$ \\
  \midrule
  $\lim$ & $\|\mathbf{v}\| \leq \pi$ \\
  \bottomrule
\end{tabular}
\end{table}

%}}}
%{{{ THE UNIT SPHERE MANIFOLD $S2$
\subsubsection{The Unit Sphere manifold $\mathbf{S2}$}
\label{ssub:the_unit_sphere_manifold_s2}

An element $x$ of $S2$ is represented by $\mathbf{x}=\psi(x)$ in $\mathbb{R}^{3}$ by:
\begin{equation}
  x\in S2,\ \mathbf{x} =\begin{bmatrix}
    x_0\\
    x_1\\
    x_2\\
  \end{bmatrix}
\end{equation}

With this manifold, the tangent space at $x$ is the tangent plane to the unit-sphere at $x$.
Thus, $T_x\mathcal{M}$ it is a 2-dimensional space, and its representation space is $\mathbb{R}^3$.
%A tangent vector to $x$, $\mathbf{z}$, is such that $\mathbf{x}\cdot \mathbf{z} = \mathbf{x}^T\mathbf{z}=0$.

For the retractation we simply use a normalized sum:
\begin{equation}
  \varphi_x(\mathbf{z}) = \frac{\mathbf{x}+\mathbf{z}}{\|\mathbf{x}+\mathbf{z}\|}
\end{equation}

We define a distance operation on $S2$ as:
\begin{equation}
  \dist(x, y) = 1-\mathbf{x}\cdot \mathbf{y}
\end{equation}

And the projection on the tangent space:
\begin{equation}
  \pi_{T_x\mathcal{M}}(\mathbf{z}) = \mathbf{z} - (\mathbf{x} \cdot \mathbf{z}) \mathbf{x}
\end{equation}

The pseudo-logarithm operation is the following:
\begin{equation}
  \zeta_x(y) = \dist(x,y)\frac{\pi_{T_x\mathcal{M}}(\mathbf{z})}{\|\pi_{T_x\mathcal{M}}(\mathbf{z})\|}
\end{equation}

The vector transport operation of vector $\mathbf{v}$ from $T_x\mathcal{M}$ to $T_y\mathcal{M}$ with $y = \varphi_x(\mathbf{v})$, corresponds to rotating $\mathbf{v}$ by the rotation that transforms $x$ into $y$:
\begin{align}
  &\mathbf{y} = \varphi_x(\mathbf{z}) \\
  &R = \mathbb{I}_3 + \widehat{\mathbf{x} \wedge \mathbf{y}} + \frac{{\widehat{\mathbf{x} \wedge \mathbf{y} } }^2}{1+\mathbf{x}\cdot\mathbf{y}} \\
  &\mathcal{T}(x,\mathbf{z}, \mathbf{v}) = R\mathbf{v}
\end{align}

\begin{table} [H]
\caption{Description of the S2 manifold}
\centering
\begin{tabular}{cc}
  \toprule
  $\mathcal{M}$ & $S2$ \\
  \midrule
  $\mathbb{E}$ & $\mathbb{R}^{3}$ \\
  \midrule
  $T_x\mathcal{M}$ & $\mathbb{R}^2$ \\
  \midrule
  $T_x\mathbb{E}$ & $\mathbb{R}^3$ \\
  \midrule
  $\varphi_x(\mathbf{z})$ & $\frac{\mathbf{x}+\mathbf{z}}{\|\mathbf{x}+\mathbf{z}\|}$ \\
  \midrule
  $\partial \varphi_x(0)$ & $\mathbb{I}_3 - \mathbf{x}\cdot\mathbf{x}^T$ \\
  \bottomrule
\end{tabular}
\quad
\begin{tabular}{cc}
  \toprule
  $\zeta(x,y)$ & $\mathbf{y} - (\mathbf{x} \cdot \mathbf{y}) \mathbf{x}$ \\
  \midrule
  $\frac{\partial \zeta_x(x)}{\partial y}$ & $\mathbb{I}_3 -\mathbf{x}\cdot\mathbf{x}^T$ \\
  \midrule
  $\mathcal{T}(x,\mathbf{z}, \mathbf{v})$ & $\mathbb{I}_3 + \widehat{\mathbf{x} \wedge \varphi_x(\mathbf{z})} + \frac{{\widehat{\mathbf{x} \wedge \varphi_x(\mathbf{z})}}^2}{1+\mathbf{x}\cdot\varphi_x(\mathbf{z})}$ \\
  \midrule
  $\pi_\mathcal{M}(\mathbf{x})$ & $\frac{\mathbf{x}}{\|\mathbf{x}\|}$ \\
  \midrule
  $\pi_{T_x\mathcal{M}}(\mathbf{z})$ & $\mathbf{z} - (\mathbf{x} \cdot \mathbf{z}) \mathbf{x}$ \\
  \midrule
  $\lim$ & $\|\mathbf{v}\| \leq \infty$ \\
  \bottomrule
\end{tabular}
\end{table}

%}}}
%{{{ CARTESIAN PRODUCT OF MANIFOLDS
\subsubsection{Cartesian Product of Manifolds}
\label{ssub:cartesian_product_of_manifolds}
Given two manifolds $\mathcal{M}_1$ and $\mathcal{M}_2$, we denote $\mathcal{M}=\mathcal{M}_1\times\mathcal{M}_2$ their cartesian product.
Any operation on an element of $\mathcal{M}$ can simply be computed term by term for each manifold composing $\mathcal{M}$.
For example, the retractation is computed as follows, and that scheme can be reproduced for all other operations:

\begin{align}
  &x_1\in \mathcal{M}_1,\ \mathbf{z_1}\in T_{x_1}\mathcal{M}_1,\ x_2\in \mathcal{M}_2,\ \mathbf{z_2}\in T_{x_2}\mathcal{M}_2\\
  &x=\begin{bmatrix}
    x_1\\x_2\\
  \end{bmatrix}\in \mathcal{M},\ \mathbf{z}=\begin{bmatrix}
    \mathbf{z_1}\\ \mathbf{z_2}\\
  \end{bmatrix}\in T_x\mathcal{M}\\
  &\varphi_x(z) = \begin{bmatrix}
    \varphi_{x_1}(\mathbf{z_1})\\
    \varphi_{x_2}(\mathbf{z_2})\\
  \end{bmatrix}
\end{align}

%}}}

\subsection{Implementation of Manifolds}
\label{sub:implementation_of_manifolds}

In order to use the manifold formulation described above in other softwares, and particularly in a numerical solver, we needed to write an independant C++ project that handles all the formulations without the user having to worry about it.
This implementation is open-source and available at \href{https://github.com/stanislas-brossette/manifolds}{https://github.com/stanislas-brossette/manifolds}.
The implementation consists in 3 types of classes: the Manifold class, the elementary manifold classes and the Point class.
The Manifold class describes the abstract mathematical structure of a non-Euclidean manifolds and defines a common interface for all elementary manifolds to implement (retractation, pseudoLog,\ldots).
Elementary Manifold classes ($\mathbb{R}^n$, $SO(3)$, $S2$, and the Cartesian Product) are the concrete manifolds.
They inherit from the Manifold class and implement all their mathematical operations.
The Cartesian Product class is used to build compound manifolds by being `multiplied' with other elementary manifolds.
The Point class represents a point on a manifold, it contains the data that represents its numerical value.
It can only be constructed by a manifold, and provides some proxy to its manifolds operations, in particular, it is equipped with an increment method, that applied a retractation on it.
Figure~\ref{fig:uml_manifold} presents a simplified class diagram of this project, ommiting all the settors, accessors, bookkeeping mechanics and accessory functions.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=\linewidth]{uml/manifolds-1.pdf}
  \caption{Simplified class diagram of the Manifold project}
\label{fig:uml_manifold}
\end{figure}

%}}}
%{{{ PRACTICAL IMPLEMENTATION
\section{Practical implementation}
\label{sec:practical_implementation}

The SQP algorithm presented in Section~\ref{local_sqp_on_manifolds} works locally, \emph{i.e.} when starting close enough to the solution.
In practice, various refinements are made to ensure convergence from any starting point.
We detail hereafter our choices.

We summarize the entire SQP algorithm in Diagrams~\ref{fig:main_sqp_loop},~\ref{fig:restoration_loop} and~\ref{fig:second_order_correction}, which represent respectively the main SQP loop, the restoration phase and the second order correction algorithm.

\subsection{Linear and quadratic problems resolution}
\label{sub:linear_and_quadratic_problems_resolution}

The central idea of an SQP algorithm is that it solves a series of QP iteratively until a solution is reached.
There are many off-the-shelf QP solvers available and the state of the art is mature in that field, thus, we decided to use the LSSOL solver~\cite{gill:techrep:1986}.
LSSOL provides resolution methods for several types of problems and we are interested in using the FP (Feasibility Problem) and QP ones.\@
The LSSOL framework formulates problems as follows (Note that there are no equality constraints in this formulation):

\begin{align}
  \minimize_{x\in \mathbb{R}^n}\ &{F(x)}\\
  \text{subject to}\  & l\leq \begin{Bmatrix}
    x\\
    Cx
  \end{Bmatrix}
  \leq u
\end{align}

With the $F$ function taking different forms depending on the problem to solve:
\begin{table} [H]
\centering
\begin{tabular}{ccc}
  \toprule
  FP:\@ & None & (find a feasible point for the constraints)\\
  \midrule
  QP2: & $F(x)=c^T+\frac{1}{2} x^T A x$ & $A$ symmetric and positive semi-definite \\
  \midrule
  QP4: & $F(x)=c^T+\frac{1}{2} x^T B^T B x$ & $B$ $m\times n$ upper-trapezoidal \\
  \bottomrule
\end{tabular}
\end{table}

The QP4 type of problem is used when a decomposition of the matrix $A$ of QP2, $A=B^T B$ is available.

\subsection{Problem Definition}
\label{sub:problem_definition}

As stated in Eq~\ref{eq:pb_on_SO3} we want to solve a non-linear constrained optimizarion problem on manifold that takes the following form:

\begin{align}
  \minimize_{x \in \mathcal{M}} & \quad f(x)\\
  \text{subject to}&
  \begin{array}{lr}
    l \leq c(x) \leq u \nonumber
  \end{array}
\end{align}

The list of constraints can be separated in 3 categories: Bounds, Linear and Nonlinear.
For convenience, we formulate our optimization problem in a way that is compatible with LSSOL by changing all equality constraints into inequality constraints:

\begin{equation}
  c(x) = 0 \Leftrightarrow 0 \leq c(x) \leq 0
\end{equation}

Our problem can be written as:
\begin{align}
\label{eq:optim_problem_on_manifold}
  \minimize_{x \in \mathcal{M}} & \quad f(x)\\ \nonumber
  \text{subject to}&\left\{
  \begin{array}{lr}
    L_B \leq x \leq U_B \\
    L_L \leq Ax \leq U_L \\
    L_N \leq c(x) \leq U_N
  \end{array}\right.
\end{align}

With $L_B$ and $U_B$ being respectively the lower and upper bounds for Bounds constraints, $L_L$ and $U_L$ the lower and upper bounds for Linear constraints, and $L_N$ and $U_N$ the lower and upper bounds for Nonlinear constraints.
That formulation is convenient and used to define our problem, but since we are solving this problem on a non-Euclidean manifold, we need to re-formulate it around the current iterate $x_i$ at each iteration.
This is done automatically behind the scene and the user only has to provide the informations to build problem~\ref{eq:optim_problem_on_manifold}.

At iteration $i$, the problem becomes (for clarity, we drop the subscript $i$ that goes with each appearance of $x$):
\begin{align}
\label{eq:optim_problem_on_txm}
  \minimize_{\mathbf{z} \in T_x\mathcal{M}} & \quad f(\varphi_x(\mathbf{z}))\\ \nonumber
  \text{subject to}&\left\{
  \begin{array}{lr}
    {\bf z}_{\text{map}}^- \leq \mathbf{z} \leq {\bf z}_{\text{map}}^+ \\
    L_B \leq \varphi_x(\mathbf{z}) \leq U_B \\
    L_L \leq A\varphi_x(\mathbf{z}) \leq U_L \\
    L_N \leq c(\varphi_x(\mathbf{z})) \leq U_N
  \end{array}\right.
\end{align}

With ${\bf z}_{\text{map}}^-$ and ${\bf z}_{\text{map}}^+$ being respectively the lower and upper bounds of validity of the tangent map of $\mathcal{M}$ around $x$.
After this reformulation, we note that if $\varphi_x$ is nonlinear, then the bounds and linear constraints of problem~\ref{eq:optim_problem_on_manifold} become nonlinear in problem~\ref{eq:optim_problem_on_txm}.
It is then necessary to treat the problem and evaluate which constraint is linear and which is not, in order to go back to a formulation of the same type as in problem~\ref{eq:optim_problem_on_manifold}.
Basically, for each constraint $c_j$ in problem~\ref{eq:optim_problem_on_txm}, if the submanifold of $\mathcal{M}$ on which that constraint is applied is a realspace $\mathbb{R}^m$ then the constraint maintains its bound, linearity or nonlinearity status, otherwise, it becomes a nonlinear constraint\footnote{At the time I am writing those lines. This treatment of the constraints is a planned work, it has not been implemented yet.}.
Once again, this may look like a drawback of optimization on manifolds, but it is usually the same with classical optimization.
Indeed, a linear constraint on the representation space of a non-Euclidean manifold might not often make much sense.
For example, a linear constraint on the elements of a unit-quaternion, although one can obviously be devised, does not seem to make much sense.

In the particular case of linear constraints (and by extension bound constraints) on a submanifold that is a realspace, the constraints on $x$ are transformed into constraints on $z$ as follows:
\begin{equation}
  L_L \leq Ax \leq U_L \ \Rightarrow \ L_L - Ax \leq A\mathbf{z} \leq U_L-Ax
\end{equation}
For bounds constraints, the same goes with $A$ being the identity matrix on the submanifold.

Once those substitutions are done, we obtain a problem of the following form (note that $L_B$, $L_L$, $L_N$, $U_B$, $U_L$, $U_N$, $A$ and $c$ are not the same as in problem~\ref{eq:optim_problem_on_manifold}):

\begin{align}
\label{eq:optim_txm_final}
  \minimize_{\mathbf{z} \in T_x\mathcal{M}} & \quad f \circ \varphi_x(\mathbf{z})\\ \nonumber
  \text{subject to}&\left\{
  \begin{array}{lr}
    L_B \leq \mathbf{z} \leq U_B \\
    L_L \leq A \mathbf{z} \leq U_L \\
    L_N \leq c \circ \varphi_x(\mathbf{z}) \leq U_N
  \end{array}\right.
\end{align}

This problem is an exact reformulation of problem~\ref{eq:optim_problem_on_manifold} on $T_x\mathcal{M}$.
At each step of the optimization process, we solve the QP approximating this problem~\ref{eq:optim_txm_final} around $\mathbf{z}=0$.
The Taylor development of $c\circ\varphi_x(z)$ to the first order around $\mathbf{z} = 0$ gives:
\begin{equation}
  c \circ \varphi_x(\mathbf{z}) \approx c\circ \varphi_{x_k}(0) + \frac{\partial c\circ \varphi_{x_k}}{\partial {\bf z}}(0) {\bf z } = c(x_k) + {\left(\nabla\varphi_{x_k}(0) \nabla c (x_k)\right)}^T\mathbf{z}
\end{equation}

The QP to solve at each iteration is the following:

\begin{align}
  \label{eq:QP_txm}
  \begin{split}
  \minimize_{\mathbf{z} \in T_x\mathcal{M}=\mathbb{R}^n } & \quad {\left(\nabla\varphi_{x_k}(0) \nabla f (x_k)\right)}^T\mathbf{z} + \frac{1}{2} {\bf z }^T H_k{\bf z }\\
  \text{subject to}&\left\{
  \begin{array}{lr}
    L_B \leq \mathbf{z} \leq U_B \\
    L_L \leq A \mathbf{z} \leq U_L \\
    L_N \leq c(x_k) + {\left(\nabla\varphi_{x_k}(0) \nabla c (x_k)\right)}^T\mathbf{z}\leq U_N\\
  \end{array}\right.
  \end{split}
\end{align}

The resolution of this QP~\ref{eq:QP_txm} (with LSSOL) gives the optimal solution $\mathbf{z}^*$ and Lagrange multipliers $\lambda^*$.
The new iterate that will be considered is:
\begin{equation}
  x_{k+1}\leftarrow \varphi_{x_k}(\mathbf{z}^*)
\end{equation}

Note that the only additional computation due to the formulation on manifold is the multiplication of the gradients of constraints and cost by the map's gradient at $0$.
The computation time related to those products can be reduced by taking advantage of the fact that $\nabla \varphi_x(0)$ is block diagonal.

\subsection{Trust-region and limit map}
\label{sub:trust_region_and_limit_map}

Maps $\varphi_{x}$ are only valid locally, and we need to account for this: a step ${\bf z}$ found by solving Problem~(\ref{eq:QP_txm}) should not be outside the validity region of the map.
We can enforce this by adding the limits of the manifold map to the problem as bound constraints.
In fact, this comes down to intersecting the original problem's bounds with the limits of the map and imposing the resulting bounds as constraints.
This leads naturally to trust region methods that we therefore favor over line-search approaches.
We present the classical trust-region strategy in Section~\ref{ssec:the_trust_region_strategy}.

In the case of a robotics problem, different variables often have different orders of magnitude, for example, contact forces can be of the order of $100N$ while joint angles are of the order of $1rad$.
Let $x$ be the variable vector of such problem with $x={[\theta_0, \theta_1, f_0, f_1, f_2]}^T$ where $\theta$ represents an angular variables and $f$ a force.
The shape of the trust-region should reflect those differences, but we want to keep the simplicity of the classical trust-region strategy.
For that, we propose to separate the trust-region $\rho$ in two parts: its scale $\rho_\text{scale}$ which is a scalar, and its shape $\rho_\text{shape}$ which in a vector of dimension $n$ (that value is actually stored in the instance of manifold).
$\rho_\text{shape}$ is set at the beginning of the optimization and remains constant, it represents the size of the trust-region in every directions.
In the previous example, we'd get ${\rho_\text{shape}} = {[1,1,100,100,100]}^T$
And $\rho_\text{scale}$ is the scalar that gets updated in the trust-region strategy.

Finally, the constraints related to the trust-region added to the problem come down to:
\begin{equation}
  -\rho = -\rho_\text{scale}\rho_\text{shape} \leq \mathbf{z} \leq \rho_\text{scale}\rho_\text{shape} = \rho
\end{equation}

\subsection{Filter method}
\label{sub:filter_method}

To know if a step ${\bf z}$ is acceptable or not, one usually uses a penalty-based merit function as we present in Section~\ref{sec:resolution_of_a_non_linear_constrained_optimization_problem}.
In our early tests, the update of the penalty parameters proved to be difficult with our types of problems.
We now use a filter approach instead, as presented in Section~\ref{ssub:the_filter_method}.

Our algorithm is an adaptation of Fletcher's filter SQP~\cite{fletcher:mathprog:2000} to the case of manifolds: we use an adaptive trust-region that is intersected with the validity region of $\varphi_{x_i}$, and a new iterate $x_{i+1} = \varphi_{x_i}({\bf z})$ is accepted if either the cost function or the sum of constraint violations is made better than for any previous iterates.

\subsection{Convergence criterion}
\label{sub:convergence_criterion}

An iterate $\{x,\lambda\}$ is considered to be a solution of the optimization problem is it satisfies the first-order optimality conditions (presented in Section~\ref{sub:optimality_conditions}) to within certain tolerances.
We use the same criterion as the one presented in~\cite{gill:snopt:2002}.
It has the advantage of using two different tolerance constants: $\tau_P$ monitores the primal optimality conditions, which means the satisfaction of the constraints, and $\tau_D$ monitores the dual conditions, which means the optimality of the cost function and of the Lagrange multipliers.
That gives the user more control over the quality of the solution.

Dropping the distinction between Linear and NonLinear Constraints,
the problem can be rewritten as:

\begin{align}
\begin{split}
\label{eq:problem}
  \min_{\bf x \in \mathcal{M}} & \quad \mbox{\emph{f}}({\bf x}) \\
  \text{subject to }&
  l \leq c({\bf x}) \leq u \\
\end{split}
\end{align}

Each constraint is considered as a double inequality, thus, is associated with two Lagrange multipliers: $\lambda_l$ for the lower bound and $\lambda_u$ for the upper bound.
The basic KKT conditions for that problem writes as:

\begin{equation}
\label{KKT}
\left\{
\begin{array}{lr}
  \nabla \mathcal{L} = \nabla f({\bf x}) + \lambda_l.\nabla c({\bf x}) + \lambda_u.\nabla
c({\bf x}) = 0 \\
  c({\bf x}) - l \geq 0 \\
  c({\bf x}) - u \leq 0 \\
  \lambda_l \leq 0 \\
  \lambda_u \geq 0 \\
  \lambda_l.(c({\bf x})-l) = 0 \\
  \lambda_u.(c({\bf x})-u) = 0
\end{array}
\right.
\end{equation}

For each constraint, there are 3 possible situations:

\begin{tabular}{cccc}
  \toprule
  Lower bound violated or active & $c(x)-l\leq0$ & $\lambda_l\leq0$ & $\lambda_u=0$ \\
  \midrule
  Constraint satisfied & $l\leq c(x) \leq u$ & $\lambda_l=0$ & $\lambda_u=0$ \\
  \midrule
  Upper bound violated or active & $c(x)-u\geq0$ & $\lambda_l=0$ & $\lambda_u\geq0$ \\
  \bottomrule
\end{tabular}

Both $\lambda_l$ and $\lambda_u$ cannot be non null at the same time. So for each constraint, we can use a single Lagrange multiplier that is negative when the lower bound is violated or active, null when the constraint is satisfied, and positive when the upper bound is active or violated.
This allows to reduce the KKT system to the following:

\begin{equation}
\label{KKTmodified}
\left\{
\begin{array}{lr}
  \nabla \mathcal{L} = \nabla f({\bf x}) + \lambda.\nabla c({\bf x}) = 0 \\
  \begin{cases}
  c({\bf x}) = l & \text{and } \lambda \leq 0 \\
  \text{OR}\\
  l \leq c({\bf x}) \leq u & \text{and } \lambda = 0 \\
  \text{OR}\\
  c({\bf x}) = u & \text{and } \lambda \geq 0
  \end{cases}
\end{array}
\right.
\end{equation}

To evaluate the satisfaction of this system, we approximate it with the tolerance constants.
First we scale the tolerance constants with respect to the values of the iterate $x$ and $\lambda$:

\begin{align}
\begin{split}
  \tau_x = \tau_P(1+\|x\|_\infty)\\
  \tau_\lambda =\tau_D(1+\|\lambda\|_\infty)\\
\end{split}
\end{align}

And we get the following convergence criterion:

\begin{equation}
\label{KKTfinal}
\left\{
\begin{array}{lr}
  \|\nabla \mathcal{L}\|_\infty \leq \tau_\lambda \\
  \forall i\
  \left\{
  \begin{array}{lll}
  |c_i({\bf x}) - l_i| \leq \tau_x & \text{and } \lambda_i \leq -\tau_\lambda\\
  \text{OR}\\
  -(c_i({\bf x}) - l_i) \leq -\tau_x &\text{and } -(c_i({\bf x}) - u_i) \geq -\tau_x & \text{and } |\lambda| \leq \tau_\lambda \\
  \text{OR}\\
  |c_i({\bf x}) - u_i| \leq \tau_x & \text{and } \lambda_i \geq \tau_\lambda\\
  \end{array}
  \right.
\end{array}
\right.
\end{equation}

\subsection{Feasibility restoration}
\label{sub:feasibility_restoration}

Along the optimization process, the set of linearized constraints in the QP Problem~\ref{eq:QP_txm} can become infeasible.
For example after a reduction of the trust-region, see Section~\ref{ssub:the_trust_region_strategy}.
In that case the QP~\ref{eq:QP_txm} cannot be solved and the resolution as presented above cannot proceed.
To quope with that issue, the algorithm enters a phase called of restoration, that aims at finding a feasible point without regards for the value of the cost function.
The basic idea is the following: At the beginning of an iteration, we estimate a list of infeasible linearized constraints.
The infeasible constraints are removed from the restoration problems constraints list and their violation is added to its cost function (that becomes the sum of all constraints violation).
Then the restoration problem only contains feasible constraints and has to minimize the sum of constraint violation.
That is done by iterating just like in the SQPs main loop with the difference that at each iteration, we update the problem based on a new list of infeasible constraints.
The restoration phase has its own filter called the restoration filter (RF) and trust region.
For more details on the restoration phase, see Section~\ref{sub:restoration_phase}.

Note that each iteration of the main SQP algorithm starts with the resolution of an FP (Feasibility Problem), that consists of the same linearized constraints as the QP problem~\ref{eq:QP_txm} without cost function.

\begin{align}
  \label{eq:FP_txm}
  \begin{split}
  \text{find } \mathbf{z}\ \text{such that:}&\left\{
  \begin{array}{lr}
    L_B \leq \mathbf{z} \leq U_B \\
    L_L \leq A \mathbf{z} \leq U_L \\
    L_N \leq c(x_k) + {\left(\nabla\varphi_{x_k}(0) \nabla c (x_k)\right)}^T\mathbf{z}\leq U_N\\
  \end{array}\right.
  \end{split}
\end{align}

Its role is to determine whether the set of linearized constraints is feasible.
If it is, the main SQP continues, otherwise, a list of infeasible constraint is provided and the restoration phase is entered.
This feasibility problem is also solved at the beginning of each restoration iteration to determine whether or not to exit the restoration phase. As soon as a feasible point is found, the restoration is exited.
Once a feasible point $x_F$ is found by the restoration phase, it is used as the new iterate in the main SQP phase.
Since during the restoration no care is taken about the value of the cost function, it is possible that $x_F$ is refused by the main filter, which is not an acceptable behavior.
So $x_F$ is forced in the filter, and any pair dominating it is removed.
Then the optimization can continue.

\subsection{Second Order Correction}
\label{sub:second_order_correction}

In the event where a step proposed in the restoration process by the resolution of the restoration QP is refused by the restoration filter RF, instead of immediately reducing the size of the trust region, we can perform a Second Order Correction Step.

The idea is to re-solve a QP after its solution $\mathbf{z}$ has been refused by RF, but with a better approximation (second order) of the constraints:

\begin{equation}
  c_i(x+{\bf z}) = c_i(x) + {\nabla c_i(x)}^T {\bf z} + \frac{1}{2}{\bf z}^T{\nabla}^2c_i(x){\bf z}
\end{equation}

The restoration QP problem becomes:

\begin{align}
  \minimize_{{\bf z} \in \mathbb{R}^n} &\ \sum_{i\in \mathcal{U}} {\nabla c_i}^T{\bf z} + \frac{1}{2}{{\bf z}^T H{\bf z}} \nonumber
  \\ \text{s.t.}&
  \left\{
  \begin{array}{lr}
    { L_B \leq {\bf z} \leq U_B}\\
    \forall i \in \mathcal{F} &\ {LB_i - c_i(x)\leq {\nabla c_i(x)}^T.{\bf z} + \frac{1}{2}{\bf z}^T{\nabla}^2c_i(x){\bf z} \leq UB_i - c_i(x)}\\
  \end{array}
  \right.
\end{align}

Using $ \frac{1}{2}{\bf z}^T{\nabla}^2c_i(x){\bf z} \approx c_i(x+{\bf z}) - c_i(x) - {\nabla c_i(x)}^T {\bf z} $
we get:

\begin{align}
  \minimize_{{\bf z} \in \mathbb{R}^n} &\ \sum_{i\in \mathcal{U}} {\nabla c_i}^T{\bf z} + \frac{1}{2}{{\bf z}^T H{\bf z}} \nonumber\\
  \text{s.t.}&
  \left\{
  \begin{array}{lr}
    L_B \leq {\bf z} \leq U_B \\
    \forall i \in \mathcal{F},\quad {LB_i - c_i(x +{\bf z}) + {\nabla c_i(x)}^T{\bf z} \leq {\nabla c_i(x)}^T{\bf z} \leq UB_i - c_i(x +{\bf z}) + {\nabla c_i(x)}^T{\bf z}}\\
  \end{array}
  \right.
\end{align}

This system is solved iteratively by updating $\bf z$ (but never changing $x$) with the solution of the previous system and updating the values in $c_i(x+\mathbf{z})$ until a satisfactory $\bf z$ is found.

\subsection{Hessian update on manifolds}
\label{sub:hessian_update_on_manifolds}

Aside from the manifold adaptation, our main departure from Fletcher is in the Hessian computation where we used an approximation, since the exact one is too expensive to compute in our problems.
After testing several possibilities, we settled for a self-scaling damped BFGS update~\cite{nocedal:mp:1993,nocedal:book:2006}, adapted to the manifold framework.
More precisely, given the Hessian approximation $H_k$ at iteration $k$, we compute the approximation $H_{k+1}$ as follows
\begin{align}
  &s_k = \mathcal{T}_z(z), \quad y_k = \nabla_z \mathcal{L}_{x_{k+1}}(0,\lambda_{k+1}) - \mathcal{T}_z(\mathcal{L}_{x_{k}}(0,\lambda_{k})) \nonumber\\
  &\theta_k = \left\{\begin{array}{ll}
    1 & \mbox{if} \; s_k^T y_k \geq 0.2 s_k^T \tilde{H}_k s_k \\
    \frac{0.8 s_k^T \tilde{H}_k s_k}{s_k^T \tilde{H}_k s_k - s_k^T y_k} & \mbox{otherwise}
  \end{array}\right. \nonumber\\
  &r_k = \theta_k y_k + \left(1-\theta_k\right) \tilde{H}_k s_k \quad \mbox{(damped update)} \nonumber\\
  &\tau_k = \min\left(1, \frac{s_k^T r_k}{s_k^T \tilde{H}_k s_k} \right) \quad \mbox{(self-scaling)} \nonumber\\
  &H_{k+1} = \tau_k \left(\tilde{H}_k-\frac{\tilde{H}_k s_k s_k^T \tilde{H}_k}{s_k^T \tilde{H}_k s_k} \right) + \frac{r_k r_k^T}{s_k^T r_k} \nonumber%
\end{align}
where $\mathcal{T}_{\bf z}$ is a vector transport along ${\bf z}$ (see~\cite{absil:book:2008}) and $\tilde{H}_k$ is such that for ${\bf u} \in T_{x_{k+1}} \mathcal{M}$, $\tilde{H}_k {\bf u} = \mathcal{T}_{\bf z}\left(H_k \mathcal{T}_{\bf z}^{-1}({\bf u}) \right)$.

\subsection{Hessian Regularization}
\label{sub:hessian_regularization}

Despite Powell's update, $H_{k}$ might not be positive definite (but still symmetric).
We regularize it as follows: we first perform a Bunch-Kaufman factorization $P_k H_k P_k^T= L_k B_k L_k^T$ where $P_k$ is a permutation matrix, $L_k$ is unit lower triangular and $B_k$ is block diagonal with blocks of size $1 \times 1$ or $2\times 2$ (obtaining $B_k$ as a diagonal matrix is not numerically stable for Cholesky-like decomposition of indefinite matrices), see~\cite{golub:book:1996}.
The eigenvalue decomposition $B_k = Q_k D_k Q_k^T$ is immediate and cheap to compute.
From the diagonal matrix $D_k$ we form $D'_k$ such that $d'_{ii} = \max\left(d_{ii},\mu_{\min}\right)$ where $\mu_{\min}>0$ is user-defined (we typically set it to $0.1$).
Defining $L'_k = L_k Q_k {(D'_k)}^{1/2}$, we get a regularized matrix $H'_k = P_k^T L_k L_k^T P_k$.
In our case, we use {\tt LSSOL}~\cite{gill:techrep:1986} for solving the QP~(\ref{eq:SQPStep}), which directly accepts the factorized form $(P_k, L'_k)$.
This avoids an internal Cholesky factorization so that our regularization does not add too much time to the overall process of building and solving the QP.\@

\subsection{An alternative Hessian Approximation Update}
\label{sub:an_alternative_hessian_approximation_update}

In~\cite{Fletcher:ifip:2006}, Fletcher presents a new Hessian update method that maintains and update the Hessian $H$ in the form $H=U U^T$, which is obviously always symmetric and it ensures that it is always positive semi-definite.
Since a decomposition of $H$ is readily available with this method, we can get rid of the regularization of the Hessian in our algorithm.
The matrix $U$ of size $(n,m)$ is smaller than $H$ and contains only informations from the last $m$ iteration, this means that this method has a built-in limited memory capability.
In our robotics problems, using limited memory Hessian updates is useful because along the iteration process, the variables may change a lot, and the Hessian value near the solution may be very different from the one approximated around the first iterations.
Thus, it can be helpful to `forget' about the old components of the approximation to leave room for the latest ones.
At each iteration, the matrix $U$ is updated with either BFGS, or SR1, or a hybrid method, based on some tests on the value of the latest step.
When possible, the SR1 update is preferred, because some results suggest that it allows faster convergence when the SR1 denominator is positive, otherwise, a BFGS or the hybrid method are used.
For all those reasons, this update method is very attractive.
We implemented it in our solver, but so far the results have not been very conclusive and some more work will be dedicated to that issue in the future.
As of now, the best results where observed when using the self-scaling BFGS update method.

\subsection{Hessian Update in Restoration phase}
\label{sub:hessian_update_in_restoration_phase}

The hessian $H$ computed through whichever method presented above is meant to approximate the value of $\nabla \mathcal{L}_{xx}(x,\lambda) = \nabla f(x) + \lambda^T \nabla c(x)$, with $f$ the cost function of the current problem to solve, and $c$ its constraints.
But during the restoration phase, the definitions of $f$ and $c$ change at each iteration, depending on the sets of infeasible and feasible constraints ($\mathcal{I}$ and $\mathcal{F}$).
In order to account for that change, we propose to compute an approximation of each constraint separately, denoting $H_i$ the hessian of $c_i$, and correctly combining the hessians based on the current set of feasible constraints.

\begin{align}
  \begin{split}
    H_f \approx \nabla^2 f\\
    H_{c_i} \approx \nabla^2 c_i\\
    H = \sum_{i\in \mathcal{I}}H_{c_i} + \sum_{i\in \mathcal{F}}\lambda_i H_{c_i}
  \end{split}
\end{align}

With this definition of $H$, we can ensure the symmetry of $H$, but nothing guarantees its positive definiteness, although each of the underlying $H_i$ are positive semi-definite.
It is then necessary to regularize $H$ after combining the $H_{c_i}$.
This allows to have an approximation of the Hessian when exiting the restoration phase, by computing $H = H_f + \sum \lambda_i H_{c_i}$ and regularizing it.

\subsection{Diagrams of the algorithms}
\label{sub:diagrams_of_the_algorithms}

\begin{figure}[htpb]
  \centering
  \input{Chapter3-Manifolds/Figs/tikzDiagrams/mainSQP}
  \caption{Main SQP Loop}
\label{fig:main_sqp_loop}
\end{figure}

\begin{figure}[htpb]
  \begin{minipage}{.5\textwidth}
    \centering
    \input{Chapter3-Manifolds/Figs/tikzDiagrams/restoration}
    \caption{Restoration Loop}
\label{fig:restoration_loop}
  \end{minipage}%
  \begin{minipage}{.5\textwidth}
    \centering
    \input{Chapter3-Manifolds/Figs/tikzDiagrams/secondOrder}
    \caption{Second Order Correction}
\label{fig:second_order_correction}
  \end{minipage}%
\end{figure}

%}}}


